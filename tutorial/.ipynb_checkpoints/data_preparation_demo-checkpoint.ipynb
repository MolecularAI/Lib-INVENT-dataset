{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **How to run this notebook (command-line)?**\n",
    "1. Install the `lib_invent_data` environment:\n",
    "`conda env -f environment.yml`\n",
    "2. Activate the environment:\n",
    "`conda activate lib_invent_data`\n",
    "3. Execute `jupyter`:\n",
    "`jupyter notebook`\n",
    "4. Copy the link to a browser\n",
    "\n",
    "\n",
    "# `Lib-INVENT Datasets`: Data preparation and statistics demo\n",
    "This demo illustrates how to compute the distribution of the chemical properties and/or filter molecule data in SMILES format to only include drug-like molecules. The second tutorial provided in this repository demonstrates the process of slicing the filtered dataset according to reaction rules to prepare it for the training of the decorator model.\n",
    "\n",
    "The specific parameters of the configuration correspond to those used in the preparation of the dataset used for training of the Lib-INVENT model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "> **There are a number of reasons to pre-process the data used for training a generative model.**\n",
    "1. Removal of invalid or duplicated entries.\n",
    "2. Removal of unusual compounds that are clearly not drug-like (too big, reactive groups and etc.). There is normally no point training model on such examples since that bias will reflected by the generative model. \n",
    "3. Removal of rare tokens. There are rare compounds that can be seen as outliers. They in turn might contain rare tokens. Excluding them frees a slot in the vocabulary and makes it smaller. Smaller vocabulary means faster training and less memory. As a result removing compounds that introduce rare tokens to the vocabulary speeds up the generative model.\n",
    "\n",
    "### Introduction\n",
    "This configuration can be used for preparing data to only include drug-like molecules or calculate stats for sliced datasets. This Demo mainly focuses on preparing and filtering data.\n",
    "> **The rules used for filtering data:**\n",
    "- 2 <= num heavy atoms <= 70   \n",
    "- allowed elements: [6, 7, 8, 9, 16, 17, 35]  \n",
    "- remove salts, neutralize charges, sanitize\n",
    "- remove side chains with 5 or more carbon atoms\n",
    "- 0<num_rings <= 10\n",
    "- num_atoms >= 6\n",
    "- mol_weights <= 760\n",
    "- num_aromatic_rings <= 8\n",
    "- heteroatom_ratio > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dependencies\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import tempfile\n",
    "\n",
    "# --------- change these path variables as required\n",
    "data_path = \"<path/to/chembl/smiles>\" # for the training of Lib-INVENT, we used ChEMBL 27 and converted to SMILES using RDKit.\n",
    "output_directory = \"<path/to/your/output_directory/>\"\n",
    "\n",
    "# --------- do not change\n",
    "# get the notebook's root path\n",
    "try: ipynb_path\n",
    "except NameError: ipynb_path = os.getcwd()  \n",
    "\n",
    "# if required, generate a folder to store the results\n",
    "try:\n",
    "    os.mkdir(output_directory)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the configuration\n",
    "`Lib-INVENT datasets` has an entry point that loads a specified `JSON` file on startup. `JSON` is a low-level data format that allows to specify a fairly large number of parameters in a cascading fashion very quickly. The parameters are structured into *blocks* which can in turn contain blocks or simple values, such as *True* or *False*, strings and numbers. In this tutorial, we will go through the different blocks step-by-step, explaining their purpose and potential values for given parameters. Note, that while we will write out the configuration as a `JSON` file in the end, in `python` we handle the same information as a simple `dict`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Run Type block\n",
    "The first block to add to the configuration specifies the `run_type`, which determines what operations should be performed. A complete list of the run types implemented can be found in the Lib-INVENT-Dataset>data_preparation>emuns>running_mode_enum.py script.\n",
    "\n",
    "The aim of this tutorial is to demonstrate the process of data purging. Since it is based on the chemical properties of the compounds, the filtering is implemeted within the `stats_extraction` run type. This choice has been made for efficiency purposes as computing the properties of the large training datasets becomes very expensive and thus should optimally not need to be performed multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the dictionary\n",
    "configuration = {\n",
    "    \"run_type\": \"stats_extraction\"                                          \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Parameters block\n",
    "\n",
    "The purpose of the second block of the configuration is to specify the parameters according to which the operation should be performed.\n",
    "\n",
    "As a first step, the appropriate paths to data have to be specified.\n",
    "- `data_path` gives a path to the input dataset. <br> \n",
    "-- if `mode` is set to `sliced_data`, this is either a tab separated file containing the sliced dataset (with the entries on each line corresponding to scaffold, decorations separated by the separator token \"|\" and the original complete compound)<br>\n",
    "-- if `mode` is set to `orig_data`, single column data of unsliced SMILES strings is expected.\n",
    "- `output_path` specifies a directory in which all the output is saved. If the flder does not exist, it is created.\n",
    "- `columns` takes a list of strings as an argument and specifies which columns are analysed. For single column data, the expected input is `original`. For sliced data,`scaffolds` and `decorations` may also be passed in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"mode\": \"orig_data\"\n",
    "    \"data_path\": data_path,                 # location to store input data      \n",
    "    \"output_path\": output_dir,              # directory where the output is saved\n",
    "    \"columns\": [\"original\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the properties to be computed are specified. The distribution of each of the computed properties (in the form of a histogram or frequency data) is saved to the output folder. \n",
    "- `properties` takes a list of molecular properties to be computed for every compound in the analysed column(s). The possible inputs are `['mol_wts', 'num_rings','num_aromatic_rings', 'num_atoms', 'hbond_donors', 'hbond_acceptors', 'hetero_atom_ratio', 'num_tokens'].`\n",
    "- `token_distribution` is a boolean parameter specifying whether a histogram of tokens present in the dataset is computed. This is computationally expensive so it isrecommended to skip if not required.\n",
    "- `token_atom_ratio` is an indicator of how complicated the SMILES string is (since compliated molecules contain many brackets and other special symbols, which increases the ratio). The parameter takes boolean input specifying wther this should be computed as it can be computationally expensive.\n",
    "- `count_decorations` is a boolean parameter relevant for sliced data. The number of decorations per scaffold can be computed to determine the distribution of the numbers of attachment points resulting from slicing.\n",
    "- If `plotting` is set to `True`, the distribution of each property is plotted and saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters.update({\n",
    "    \"properties\": ['mol_wts',               \n",
    "                   'num_rings',              \n",
    "                   'num_aromatic_rings',     \n",
    "                   'num_atoms',\n",
    "                   'hbond_donors',\n",
    "                   'hbond_acceptors',\n",
    "                   'hetero_atom_ratio'],\n",
    "    \"token_distribution\": True,    \n",
    "    \"token_atom_ratio\": True,\n",
    "    \"count_decorations\": False,               # For sliced data only.\n",
    "    \"plotting\": True\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `orig_data` mode, the data is always standardised upon loading. This is performed using the RDKitStandardizer provided by the `reinvent-chemistry` package.\n",
    "\n",
    "`Standardization_config` contains rules to standardise the molecules using functions in reinvent chemistry.  The default filter, as implemented in version 0.0.32 of the package, was used for the preprocessing in the Lib-INVENT publication. This standardization includes:\n",
    "- 2 <= num heavy atoms <= 70\n",
    "- allowed elements: [6, 7, 8, 9, 16, 17, 35]\n",
    "- remove salts, neutralise charges, sanitize\n",
    "- remove side chains with 5 or more carbon atoms\n",
    "\n",
    "To save the standardised dataset, set the option `save_standardised` to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters.update({\n",
    "    \"standardisation_config\":{[]},            # No argument amounts to a default filter\n",
    "    \"save_standardised\": True,  \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final part of the parameters block of the configuration specifies the filtering rules, if applied.\n",
    "- `filter` takes a dictionary as a parameter. An empty dictionary indicates no filtering; otherwise, the keys correspond to the properties while the values are lists specifying the filter criteria. The first entry of the list is either `max` or `min`, determining which boundary is applied (corresponding to 'greater/smaller than *or equal to* the value). The second entry is the value of the boundary. <br>\n",
    "-- if two-sided bounds are to be imposed, they should be passed as two separate filters.<br>\n",
    "-- any property passed as a key to the filter must have been previously calculated. If it is missing in the `properties` list, the filter reports an error.\n",
    "- `save_cut_precomuted` is a boolean parameter specifying whether the filtered datset should be saved. This is typically the case for data purging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters.update({\n",
    "    \"filter\": {                             \n",
    "        \"num_rings\": [\"max\", 10],           \n",
    "        \"num_rings\": [\"min\", 1],\n",
    "        \"num_atoms\": [\"min\", 6],\n",
    "        \"mol_wts\": [\"max\", 760],\n",
    "        \"num_aromatic_rings\": [\"max\", 8],\n",
    "        \"hetero_atom_ratio\": [\"min\", 0.5],\n",
    "        \"token_atom_ratio\": [\"max\", 2]\n",
    "    },                                    \n",
    "                                                          \n",
    "    \"save_cut_precomputed\": True\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble and save the configuration\n",
    "\n",
    "With theparameters block prepared, the configuration can be assembled and saved as a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration.update({\"parameters\": parameters})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the configuration file to the disc\n",
    "configuration_JSON_path = os.path.join(output_directory, \"data_preparation_example.json\")\n",
    "with open(configuration_JSON_path, 'w') as f:\n",
    "    json.dump(configuration, f, indent=4, sort_keys= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run\n",
    "Execute in jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture captured_err_stream --no-stderr \n",
    "\n",
    "# execute\n",
    "%cd </path/to/Lib-DESIGN-datasets/project/directory/>\n",
    "!spark-submit --driver-memory=32g --conf spark.driver.maxResultSize=16g input.py {configuration_JSON_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the output to a file, just to have it for documentation\n",
    "with open(os.path.join(output_directory, \"run.err\"), 'w') as file:\n",
    "    file.write(captured_err_stream.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute in command line\n",
    "```\n",
    "# activate envionment\n",
    "conda activate lib_invent_data\n",
    "\n",
    "# go to the root folder of input.py \n",
    "cd </path/to/Lib-INVENT-datasets/directory>\n",
    "\n",
    "# execute in command line\n",
    "spark-submit --driver-memory=32g --conf spark.driver.maxResultSize=16g input.py </path/to/configuration.json>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducing the Data Purging from Lib-INVENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing of the ChEMBL 27 dataset for the use with the Lib-INVENT project has been performed in multiple stages. Upon each iteration, the distribution of the resulting dataset was examined and more filters imposed as necessary to remove outliers. This process amounted to gradually implementing all of the filters as given in the input above.\n",
    "\n",
    "In the final stage, the remaining non-drug like compounds were removed manually from the command line, using the `grep` command. The following patterns were removed since they were undesirable and too rare to be learnt by the model:\n",
    "\n",
    "`N=[N+]=[N-], [N-]=[N+]=N, [o+], [C-], [C+], %10, [c+], [cH-], [NH-], [c-], [O+], [CH-], [CH], [N], [SH2], [CH2]`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
